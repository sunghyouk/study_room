{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. word2vec 속도 개선\n",
    "\n",
    "가장 큰 문제 - 말뭉치에 포함된 어휘 수가 많아지면 계산량도 커진다.  \n",
    "\n",
    "1) `Embedding` - 새로운 계층을 추가\n",
    "2) `negative sampling` - 새로운 손실함수를 도입\n",
    "\n",
    "`PTB dataset`을 가지고 학습을 수행\n",
    "\n",
    "## 4.1 word2vec 개선 1\n",
    "\n",
    "* 입력층의 원핫 표현과 가중치 행렬 $\\mathbf{W}_{in}$의 곱 계산 -> `Embedding`으로 해결\n",
    "* 은닉층과 가중치 행렬 $\\mathbf{W}_{out}$의 곱 및 softmax 계층의 계산 -> `negative sampling`으로 해결\n",
    "\n",
    "*Reference)*  \n",
    "ch03 디렉토리의 simple_cbow.py, simple_skip_gram.py는 3장의 개선 전 간단 구현  \n",
    "ch04 디렉토리의 cbow.py, skip_gram.py는 4장의 개선 후 구현  \n",
    "\n",
    "### 4.1.1 Embedding 계층\n",
    "\n",
    "단지 행렬의 특정 행을 추출하는 것 뿐이다.  \n",
    "행렬 곱 계산은 사실 필요가 없는 것  \n",
    "`embedding` 계층에 단어 임베딩 (분산 표현)을 저장  \n",
    "\n",
    "### 4.1.2 Embedding 계층 구현\n",
    "\n",
    "GOTO: common/layers.py  \n",
    "\n",
    "## 4.2 word2vec 개선 2\n",
    "\n",
    "### 4.2.1 은닉층 이후 계산의 문제점\n",
    "\n",
    "* 은닉층의 뉴런과 가중치 행렬의 곱\n",
    "* softmax 계층의 계산\n",
    "\n",
    "### 4.2.2 다중 분류에서 이진 분류로\n",
    "\n",
    "### 4.2.3 시그모이드 함수와 교차 엔트로피 오차\n",
    "\n",
    "다중 분류를 이진 분류로 근사하는 것: sigmoid with loss  \n",
    "loss는 cross entropy error 함수  \n",
    "\n",
    "### 4.2.4 다중 분류에서 이진 분류로 (구현)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
