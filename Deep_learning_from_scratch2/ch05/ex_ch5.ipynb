{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. 순환 신경망 (RNN)\n",
    "\n",
    "* 피드포워드: 시계열 데이터를 잘 다루지 못한다\n",
    "\n",
    "## 5.1 확률과 언어 모델\n",
    "\n",
    "### 5.1.1 word2vec을 확률 관점에서 바라보다\n",
    "\n",
    "CBOW 모델의 본래 목적인 '맥락으로부터 타깃을 추측하는 것'은 어디에 이용할 수 있을까?  \n",
    "확률 $P(w_t|w_{t-2}, w_{t-1})$은 실용적인 쓰임이 있는가?  \n",
    "여기서 언어 모델이 등장  \n",
    "\n",
    "### 5.1.2 언어 모델\n",
    "\n",
    "언어 모델은 단어 나열에 확률을 부여  \n",
    "기계 번역과 음성 인식이 대표적인 예  \n",
    "\n",
    "확률의 **곱셈정리**: A와 B가 모두 일어날 확률 $P(A, B)$는 B가 일어날 확률 $P(B)$와 B가 일어난 후 A가 일어날 확률 $P(A|B)$를 곱한 값과 같다.  \n",
    "$$P(A, B)=P(A|B)P(B)$$\n",
    "\n",
    "### 5.1.3 CBOW 모델을 언어 모델로?\n",
    "\n",
    "맥락 크기는 얼마든지 키울 수 있지만 단어 순서가 무시된다는 한계가 있다.  \n",
    "그래서 RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 메카니즘을 갖추고 있다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 RNN이란?\n",
    "\n",
    "### 5.2.1 순환하는 신경망\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
