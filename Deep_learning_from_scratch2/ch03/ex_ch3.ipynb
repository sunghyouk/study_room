{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. word2vec\n",
    "\n",
    "## 3.1 추론 기반 기법과 신경망\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 통계 기반 기법의 문제점\n",
    "\n",
    "대규모 말뭉치를 다룰 때 문제가 발생  \n",
    "단 1회의 처리 (SVD 등)에 단어의 분산 표현: 배치 학습  \n",
    "신경망이 한번에 소량 (미니배치)의 학습 샘플씩 반복해서 학습하며 가중치를 갱신  \n",
    "게다가 여러 머신과 여러 GPU를 이용한 병렬 계산도 가능해져 학습 속도를 높일 수 있음  \n",
    "\n",
    "*reference* 3.5.3 통계 기반 vs 추론 기반"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 추론 기반 기법 개요\n",
    "\n",
    "모델로 신경망을 사용  \n",
    "모델은 맥락 정보를 입력받아 (출현할 수 있는) 각 단어의 출현 확률을 출력  \n",
    "학습의 결과로 단어의 분산 표현을 얻는 것  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 신경망에서의 단어 처리\n",
    "\n",
    "고정 길이의 벡터로 변환 - 원핫 표현  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01354781  2.74418459  0.29150569]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "W = np.random.randn(7, 3)\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 단순한 word2vec\n",
    "\n",
    "### 3.2.1 CBOW 모델의 추론 처리\n",
    "\n",
    "사용할 신경망은 word2vec에서 제안하는 CBOW (continuous bag-of-words) 모델  \n",
    "맥락 (주변 단어들)으로부터 타깃 (중앙 단어)을 추측하는 용도의 신경망  \n",
    "입력: 맥락의 원핫 표현, 맥락으로 고려할 단어 2개로 정하면 입력층도 2이다.  \n",
    "은닉: 입력층이 여러 개이면 전체를 '평균'  \n",
    "출력: 뉴런 하나하나가 각각의 단어에 대응, 출력층 뉴런은 각 단어의 '점수'를 뜻하며, 소프트맥스 함수를 적용해서 '확률'을 얻음\n",
    "\n",
    "은닉층의 뉴런 수를 입력층의 뉴런 수보다 적게 - 인코딩에 해당  \n",
    "활성화 함수를 사용하지 않는 간단한 구성의 신경망  \n",
    "\n",
    "* ch03/cbow_predict.py에 구현\n",
    "\n",
    "### 3.2.2 CBOW 모델의 학습\n",
    "\n",
    "확률이란 맥락 (전후 단어)이 주어졌을 때 그 중앙에 어떤 단어가 출현하는지  \n",
    "정답에 해당하는 뉴런의 값이 클 것이라 기대할 수 있다.  \n",
    "CBOW 모델의 학습에서 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 한다.  \n",
    "그러므로, 이 신경망을 학습하려면 소프트맥스와 교차 엔트로피 오차만 이용하면 된다.  \n",
    "\n",
    "### 3.2.3 word2vec의 가중치와 분산 표현\n",
    "\n",
    "두 가지 가중치: 입력 측 완전연결계층의 가중치 ($\\mathbf{W}_{in}$), 출력 측 완전연결계층의 가중치($\\mathbf{W}_{out}$)\n",
    "* $\\mathbf{W}_{in}$ - 각 행이 각 단어의 분산 표현\n",
    "* $\\mathbf{W}_{out}$ - 단어의 의미가 인코딩된 벡터가 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 학습 데이터 준비\n",
    "\n",
    "* 입력: 맥락의 각 행\n",
    "* 타깃: 각 행이 정답 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6] {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "# 말뭉치 텍스트를 단어 ID로 변환 (2장에서 구현한 preprocess 함수 사용)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus, id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 맥락과 타깃을 만드는 함수를 구현\n",
    "* common/util.py에 create_contexts_target 함수로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]] [1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 CBOW 모델 구현\n",
    "\n",
    "* ch03/simple_cbow.py에 구현\n",
    "* 같은 가중치가 여러 개 존재 - 옵티마이저 처리가 본래의 동작과 달라진다 (?): common/trainer.py `remove_duplicate()` 함수 참조\n",
    "* ch03/train.py - goto Chapter 1. `Trainer class`\n",
    "* common/optimizer.py 여러 알고리즘이 구현된 파일\n",
    "\n",
    "## 3.5 word2vec 보충\n",
    "\n",
    "### 3.5.1 CBOW 모델과 확률\n",
    "\n",
    "CBOW 모델이 하는 일은 맥락을 주면 타깃 단어가 출현할 확률을 출력하는 것  \n",
    "맥락으로 $w_{t-1}$과 $w_{t+1}$이 주어졌을 때 타깃이 $w_t$가 될 확률:  \n",
    "$$P(w_t|w_{t-1}, w_{t+1})$$\n",
    "\n",
    "CBOW 모델의 손실 함수도 표현 가능 (**음의 로그 가능도**, negative log likelihood)  \n",
    "$$L = -\\log{P(w_t|w_{t-1}, w_{t+1})}$$\n",
    "\n",
    "이를 말뭉치 전체로 확장하면  \n",
    "$$L = -\\frac{1}{T}\\sum_{t=1}^T\\log{P(w_t|w_{t-1}, w_{t+1})}$$\n",
    "\n",
    "이 손실 함수의 값을 가능한 한 작게 만드는 것, 이 때의 가중치 매개변수가 우리가 얻고자 하는 단어의 분산 표현\n",
    "\n",
    "### 3.5.2 skip-gram 모델\n",
    "\n",
    "CBOW에서 다루는 맥락과 타깃을 역전시킨 모델  \n",
    "`skip-gram` 모델은 중앙의 단어로부터 주변의 여러 단어를 추측  \n",
    "모델을 확률 표기로 나타내면:  \n",
    "$$P(w_{t-1}, w_{t+1}|w_t)$$\n",
    "\n",
    "`조건부 독립`이라 가정하면 분해가 가능:  \n",
    "$$P(w_{t-1}, w_{t+1} | w_t) = P(w_{t-1} | w_t) + P(w_{t+1} | w_t)$$\n",
    "\n",
    "손실 함수:  \n",
    "$$L = -(\\log{P(w_{t-1} | w_t)} + \\log{P(w_{t+1} | w_t)})$$\n",
    "\n",
    "말뭉치 전체로 확장:  \n",
    "$$L = -\\frac{1}{T}\\sum_{t=1}^T(\\log{P(w_{t-1} | w_t)} + \\log{P(w_{t+1} | w_t)})$$\n",
    "\n",
    "단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* 추론 기반 기법은 추측하는 것이 목적, 그 부산물로 단어의 분산 표현을 얻음\n",
    "* `word2vec` 추론 기반 기법\n",
    "* `word2vec`은 `skip-gram` 모델과 `CBOW` 모델을 제공\n",
    "* `CBOW` 모델은 여러 단어로부터 하나의 단어를 추측\n",
    "* `skip-gram` 모델은 하나의 단어로부터 다수의 단어를 추측\n",
    "* `word2vec`은 가중치를 다시 학습할 수 있으므로, 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수행할 수 있다.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8416a3f2a026e985bee17eceb7546469969f3a4d162bd0f7da6d9cc666a7e5f9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
