{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차제곱합 함수 만들기\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차 함수 만들기\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data 불러오기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차 함수 미니배치에 맞게 바꾸기\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 수치 미분\n",
    "\n",
    "**수치 미분**: 아주 작은 `차분`으로 근사치로 계산하는 미분 방법\n",
    "\n",
    "`차분`: 임의의 두 점에서 함수 값의 차이  \n",
    "cf) 해석적 미분은 오차를 포함하지 않는 '진정한 미분' 값을 구해 준다. 가령  \n",
    "$y = x^2$의 미분 값은 $\\frac{dy}{dx}=2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중심 차분/ 중앙 차분을 이용한 수치 미분 구현\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-4 # 무작정 작은 값은 반올림 오차로 제대로 계산되지 않는다.\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 함수 구현\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 구현\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x= init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_simplenet 불러 들이기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.07506934 -0.32563083 -0.96098297]\n",
      " [ 0.1567326  -0.67917147  0.00502923]]\n"
     ]
    }
   ],
   "source": [
    "# 정규 분포로 초기화된 2행3렬 매개변수 행렬 출력\n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12003085 -2.25618717 -0.01798226]\n"
     ]
    }
   ],
   "source": [
    "# 0.6, 0.9일 때 예측 값\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7369435262894557"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측값과 정답 셋의 차의 손실함수 값\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41081764  0.08354753 -0.49436516]\n",
      " [ 0.61622645  0.12532129 -0.74154774]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "## 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_layer_net.py 옮겨 오기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "net.params['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09915, 0.1009\n",
      "train acc, test acc | 0.78825, 0.7944\n",
      "train acc, test acc | 0.8751333333333333, 0.8787\n",
      "train acc, test acc | 0.8974833333333333, 0.9008\n",
      "train acc, test acc | 0.90835, 0.9101\n",
      "train acc, test acc | 0.91485, 0.9171\n",
      "train acc, test acc | 0.9184666666666667, 0.9224\n",
      "train acc, test acc | 0.9238333333333333, 0.9266\n",
      "train acc, test acc | 0.9269833333333334, 0.9302\n",
      "train acc, test acc | 0.9303666666666667, 0.9329\n",
      "train acc, test acc | 0.9339666666666666, 0.935\n",
      "train acc, test acc | 0.9364166666666667, 0.937\n",
      "train acc, test acc | 0.9393, 0.9402\n",
      "train acc, test acc | 0.9410166666666666, 0.9411\n",
      "train acc, test acc | 0.9440166666666666, 0.9436\n",
      "train acc, test acc | 0.94525, 0.9445\n",
      "train acc, test acc | 0.9473833333333334, 0.9465\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cf26083bf0f871e03e073e3e6573f5d39e62bc167e5b46a8608e345be5a9832"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
