{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 랜덤 포레스트의 특성 중요도 사용\n",
    "\n",
    "앙상블에 참여한 모든 결정 트리에서 계산한 평균적인 불순도 감소로 특성 중요도를 측정 가능  \n",
    "사이킷런의 랜덤 포레스트 구현은 특성 중요도 값을 수집해 놓음  \n",
    "`RandomForestClassifier` 클래스에서 `feature_importances_` 속성으로 확인  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "feat_labels = df_wine.columns[1:]\n",
    "forest = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f+1, 30, feat_labels[indices[f]],\n",
    "                            importances[indices[f]]))\n",
    "    \n",
    "plt.title('Feature importance')\n",
    "plt.bar(range(X_train.shape[1]),\n",
    "        importances[indices],\n",
    "        align='center')\n",
    "\n",
    "plt.xticks(range(X_train.shape[1]),\n",
    "           feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SelectFromModel`: 사용자가 지정한 임계 값을 기반으로 특성을 선택  \n",
    "* `coef_`\n",
    "* `feature_importances_`\n",
    "* `importance_getter` 매개변수\n",
    "* `threshold` 매개변수 (default: mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(forest, threshold=0.1, prefit=True)\n",
    "X_selected = sfm.transform(X_train)\n",
    "print('이 임계 조건을 만족하는 샘플의 수: ', X_selected.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(X_selected.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f+1, 30, \n",
    "                            feat_labels[indices[f]],\n",
    "                            importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*REFERENCE)*  \n",
    "RFE (재귀적 특성 제거 방법): 처음에 모든 특성을 사용하여 모델링, 특성 중요도가 가장 낮은 특성을 제거  \n",
    "그 다음 제외된 특성을 빼고 나머지 특성으로 새로운 모델을 만듦  \n",
    "미리 정의한 특성 개수가 남을 때까지 계속  \n",
    "\n",
    "* `n_features_to_select` 매개변수: 선택할 특성의 개수, 비율을 지정\n",
    "* `step` 매개변수: 각 반복에서 제거할 특성의 개수를 지정\n",
    "* `coef_`, `feature_importances_` 속성을 기준으로 특성을 제거\n",
    "* `importance_getter` 매개변수: 사용할 속성을 지정할 수 있음\n",
    "* `ranking_` 속성: 선택한 특성의 우선순위\n",
    "* `support_` 속성: 선택된 특성\n",
    "* `estimator_` 속성: 훈련된 기반 모델 (랜덤 포레스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(forest, n_features_to_select=5)\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.ranking_\n",
    "f_mask = rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfe.estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in indices:\n",
    "    print(\"%2d) %-*s %f\" % (f+1, 30, \n",
    "                            feat_labels[f_mask][i],\n",
    "                            importances[i]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
