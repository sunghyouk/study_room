{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunghyouk/study_room/blob/master/python_ML/ex_chapter17_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWo0GLS6PQ4D"
      },
      "source": [
        "## 0. Initial Configuration using Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEDKBZiuJ-cM",
        "outputId": "141017ac-e427-4a59-d8ab-273ef443341e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZpzLdxDOBf7",
        "outputId": "414a1947-b64f-4065-d23c-9b2f0dd5abcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU:  False\n"
          ]
        }
      ],
      "source": [
        "print('GPU: ', len(tf.config.list_physical_devices('GPU')) > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmBXpvuaOLba",
        "outputId": "6889087b-3b9a-4548-abfc-eb398131c447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/CPU:0\n"
          ]
        }
      ],
      "source": [
        "if tf.config.list_physical_devices('GPU'):\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "else:\n",
        "    device_name = '/CPU:0'\n",
        "print(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "uP5TCwL5OlMZ",
        "outputId": "466ad33a-de28-46e9-ebde-f4d97863ece1"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-122f44540f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_memory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/config.py\u001b[0m in \u001b[0;36mget_memory_info\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMemory\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m \u001b[0;34m'\"CPU:0\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \"\"\"\n\u001b[0;32m--> 573\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_memory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mget_memory_info\u001b[0;34m(self, dev)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_GetMemoryInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_memory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Allocator stats not available for device '/CPU:0'"
          ]
        }
      ],
      "source": [
        "tf.config.experimental.get_memory_info(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf3TjrXAO1Le"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsP2NRcWPHsv"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI3pepUTAwqc"
      },
      "source": [
        "### 17.2.2 생성자와 판별자 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yNuS8FSw1uSs"
      },
      "outputs": [],
      "source": [
        "# 헬퍼 함수 정의\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generator\n",
        "def make_generator_network(\n",
        "    num_hidden_layers=1,\n",
        "    num_hidden_units=100,\n",
        "    num_output_units=784):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        model.add(\n",
        "            tf.keras.layers.Dense(\n",
        "                units=num_hidden_units, use_bias=False))\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=num_output_units, activation='tanh'))\n",
        "    return model\n",
        "\n",
        "# Discriminator\n",
        "def make_discriminator_network(\n",
        "    num_hidden_layers=1,\n",
        "    num_hidden_units=100,\n",
        "    num_output_units=1):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        model.add(\n",
        "            tf.keras.layers.Dense(units=num_hidden_units))\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        model.add(tf.keras.layers.Dropout(rate=0.5))\n",
        "    \n",
        "    model.add(\n",
        "        tf.keras.layers.Dense(\n",
        "            units=num_output_units, activation=None))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReVmeIPc5OKT",
        "outputId": "1219c33b-0527-4dfe-e57d-d23d688d70be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-29 21:00:00.191670: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 100)               2000      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 784)               79184     \n",
            "=================================================================\n",
            "Total params: 81,184\n",
            "Trainable params: 81,184\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "image_size = (28, 28)\n",
        "z_size = 20\n",
        "mode_z = 'uniform'  # 'uniform' vs. 'normal'\n",
        "gen_hidden_layers = 1\n",
        "gen_hidden_size = 100\n",
        "disc_hidden_layers = 1\n",
        "disc_hidden_size = 100\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "gen_model = make_generator_network(\n",
        "    num_hidden_layers=gen_hidden_layers, \n",
        "    num_hidden_units=gen_hidden_size,\n",
        "    num_output_units=np.prod(image_size))\n",
        "\n",
        "gen_model.build(input_shape=(None, z_size))\n",
        "gen_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nuwIho8m6bNw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 78,601\n",
            "Trainable params: 78,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "disc_model = make_discriminator_network(\n",
        "    num_hidden_layers=disc_hidden_layers,\n",
        "    num_hidden_units=disc_hidden_size)\n",
        "disc_model.build(input_shape=(None, np.prod(image_size)))\n",
        "disc_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCDHuPj6Anei"
      },
      "source": [
        "### 17.2.3 훈련 데이터셋 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TYoTazYO62eJ"
      },
      "outputs": [],
      "source": [
        "mnist_bldr = tfds.builder('mnist')\n",
        "mnist_bldr.download_and_prepare()\n",
        "mnist = mnist_bldr.as_dataset(shuffle_files=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vY03htUPBDUJ"
      },
      "outputs": [],
      "source": [
        "def preprocess(ex, mode='uniform'):\n",
        "    image = ex['image']\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.reshape(image, [-1])\n",
        "    image = image * 2 - 1.0\n",
        "    if mode == 'uniform':\n",
        "        input_z = tf.random.uniform(\n",
        "            shape=(z_size, ), minval=-1.0, maxval=1.0)\n",
        "    elif mode == 'normal':\n",
        "        input_z = tf.random.normal(shape=(z_size,))\n",
        "    return input_z, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "smg2O8-LB7J-"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = mnist['train']\n",
        "mnist_trainset = mnist_trainset.map(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ni5LKnz9CQkv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-29 21:00:25.179314: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input-z -- size:     (32, 20)\n",
            "input-real -- size:  (32, 784)\n",
            "Generator output -- size:  (32, 784)\n",
            "Discriminator (Real) -- size:  (32, 1)\n",
            "Discriminator (Fake) -- size:  (32, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-29 21:00:25.814707: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "mnist_trainset = mnist_trainset.batch(32, drop_remainder=True)\n",
        "input_z, input_real = next(iter(mnist_trainset))\n",
        "print('input-z -- size:    ', input_z.shape)\n",
        "print('input-real -- size: ', input_real.shape)\n",
        "\n",
        "g_output = gen_model(input_z)\n",
        "print('Generator output -- size: ', g_output.shape)\n",
        "\n",
        "d_logits_real = disc_model(input_real)\n",
        "d_logits_fake = disc_model(g_output)\n",
        "print('Discriminator (Real) -- size: ', d_logits_real.shape)\n",
        "print('Discriminator (Fake) -- size: ', d_logits_fake.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JVGempGEqek"
      },
      "source": [
        "### 17.2.4 GAN 모델 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dy2cz_XHEWtL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator loss: 0.7355\n",
            "Discriminator loss: (Real) 0.7355 (Fake) 0.6583\n"
          ]
        }
      ],
      "source": [
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Generator loss\n",
        "g_labels_real = tf.ones_like(d_logits_fake)\n",
        "g_loss = loss_fn(y_true=g_labels_real, y_pred=d_logits_fake)\n",
        "print('Generator loss: {:.4f}'.format(g_loss))\n",
        "\n",
        "# Discriminator loss\n",
        "d_labels_real = tf.ones_like(d_logits_real)\n",
        "d_labels_fake = tf.zeros_like(d_logits_fake)\n",
        "d_loss_real = loss_fn(y_true=d_labels_real, y_pred=d_logits_fake)\n",
        "d_loss_fake = loss_fn(y_true=d_labels_fake, y_pred=d_logits_fake)\n",
        "print('Discriminator loss: (Real) {:.4f} (Fake) {:.4f}'.format(d_loss_real.numpy(), d_loss_fake.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CKzzIOjLJI62"
      },
      "outputs": [],
      "source": [
        "# 훈련하기\n",
        "import time\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "image_size = (28, 28)\n",
        "z_size = 20\n",
        "mode_z = 'uniform'\n",
        "gen_hidden_layer = 1\n",
        "gen_hidden_size = 100\n",
        "disc_hidden_layer = 1\n",
        "disc_hidden_size = 100\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "if mode_z == 'uniform':\n",
        "    fixed_z = tf.random.uniform(\n",
        "        shape=(batch_size, z_size),minval=-1, maxval=1)\n",
        "elif mode_z == 'normal':\n",
        "    fixed_z = tf.random.normal(\n",
        "        shape=(batch_size, z_size))\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "    g_output = g_model(input_z, training=False)\n",
        "    images = tf.reshape(g_output, (batch_size, *image_size))\n",
        "    return (images+1)/2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YM4uoz6edVBw"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 준비\n",
        "mnist_trainset = mnist['train']\n",
        "mnist_trainset = mnist_trainset.map(\n",
        "    lambda ex: preprocess(ex, mode=mode_z))\n",
        "mnist_trainset = mnist_trainset.shuffle(10000)\n",
        "mnist_trainset = mnist_trainset.batch(\n",
        "    batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cDAKJqwHdsRi"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'device_name' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/rw/qhxpv8ld5lz_zd0bxydq952w0000gn/T/ipykernel_42705/3788721391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 준비\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     gen_model = make_generator_network(\n\u001b[1;32m      4\u001b[0m         \u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnum_hidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device_name' is not defined"
          ]
        }
      ],
      "source": [
        "# 모델 준비\n",
        "with tf.device(device_name):\n",
        "    gen_model = make_generator_network(\n",
        "        num_hidden_layers=gen_hidden_layers,\n",
        "        num_hidden_units=gen_hidden_size,\n",
        "        num_output_units=np.prod(image_size))\n",
        "    gen_model.build(input_shape=(None, z_size))\n",
        "\n",
        "    disc_model = make_discriminator_network(\n",
        "        num_hidden_layers=disc_hidden_layer,\n",
        "        num_hidden_units=disc_hidden_size)\n",
        "    disc_model.build(input_shape=(None, np.prod(image_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1k0w2IceWpI"
      },
      "outputs": [],
      "source": [
        "# 손실 함수와 옵티마이저:\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "g_optimizer = tf.keras.optimizers.Adam()\n",
        "d_optimizer = tf.keras.optimizers.Adam()\n",
        "all_losses = []\n",
        "all_d_vals = []\n",
        "epoch_samples = []\n",
        "start_time = time.time()\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    epoch_losses, epoch_d_vals = [], []\n",
        "\n",
        "    for i, (input_z, input_real) in enumerate(mnist_trainset):\n",
        "\n",
        "        # 생성자 손실을 계산\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            g_output = gen_model(input_z)\n",
        "            d_logits_fake = disc_model(g_output, training=True)\n",
        "            labels_real = tf.ones_like(d_logits_fake)\n",
        "            g_loss = loss_fn(y_true=labels_real, y_pred=d_logits_fake)\n",
        "        \n",
        "        # g_loss의 그레이디언트 계산\n",
        "        g_grads = g_tape.gradient(g_loss, gen_model.trainable_variables)\n",
        "\n",
        "        # 최적화: 그레이디언트를 적용\n",
        "        g_optimizer.apply_gradients(\n",
        "            grads_and_vars = zip(g_grads, gen_model.trainable_variables))\n",
        "        \n",
        "        # 판별자 손실을 계산\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            # 실제 데이터\n",
        "            d_logits_real = disc_model(input_real, training=True)\n",
        "            d_labels_real = tf.ones_like(d_logits_real)\n",
        "            d_loss_real = loss_fn(y_true=d_labels_real, y_pred=d_logits_real)\n",
        "\n",
        "            # 생성자가 만든 데이터\n",
        "            d_logits_fake = disc_model(g_output, training=True)\n",
        "            d_labels_fake = tf.zeros_like(d_logits_fake)\n",
        "            d_loss_fake = loss_fn(y_true=d_labels_fake, y_pred=d_logits_fake)\n",
        "\n",
        "            # 두 손실 함수의 합\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "        \n",
        "        # d_loss의 그레이디언트 계산\n",
        "        d_grads = d_tape.gradient(d_loss, disc_model.trainable_variables)\n",
        "\n",
        "        # 최적화: 그레이디언트를 적용\n",
        "        d_optimizer.apply_gradients(\n",
        "            grads_and_vars = zip(d_grads, disc_model.trainable_variables))\n",
        "        \n",
        "        epoch_losses.append(\n",
        "            (g_loss.numpy(), d_loss.numpy(), \n",
        "             d_loss_real.numpy(), d_loss_fake.numpy()))\n",
        "        \n",
        "        d_probs_real = tf.reduce_mean(\n",
        "            tf.sigmoid(d_logits_real))\n",
        "        d_probs_fake = tf.reduce_mean(\n",
        "            tf.sigmoid(d_logits_fake))\n",
        "        \n",
        "        epoch_d_vals.append(\n",
        "            (d_probs_real.numpy(), d_probs_fake.numpy()))\n",
        "        \n",
        "    all_losses.append(epoch_losses)\n",
        "    all_d_vals.append(epoch_d_vals)\n",
        "    print(\n",
        "        '에포크 {:03d} | 시간 {:.2f} min | 평균 손실 >>'\n",
        "        ' 생성자/판별자 {:.4f}/{:.4f} [판별자-진짜: {:.4f} 판별자-가짜: {:.4f}]'\n",
        "        .format(\n",
        "            epoch, (time.time() - start_time)/60, *list(np.mean(all_losses[-1], axis=0))))\n",
        "    epoch_samples.append(create_samples(gen_model, fixed_z).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwqyUy6LlbTn"
      },
      "outputs": [],
      "source": [
        "# 그래프로 출력 두 신경망의 훈련 과정을 분석하고 수렴하는지 평가\n",
        "import itertools\n",
        "fig = plt.figure(figsize=(16, 6))\n",
        "# 손실 그래프\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "g_losses = [item[0] for item in itertools.chain(*all_losses)]\n",
        "d_losses = [item[1]/2.0 for item in itertools.chain(*all_losses)]\n",
        "\n",
        "plt.plot(g_losses, label='Generator loss', alpha=0.95)\n",
        "plt.plot(d_losses, label='Discriminator loss', alpha=0.95)\n",
        "plt.legend(fontsize=20)\n",
        "ax.set_xlabel('Iteration', size=15)\n",
        "ax.set_ylabel('Loss', size=15)\n",
        "\n",
        "epochs = np.arange(1, 100+1)\n",
        "epoch2iter = lambda e: e*len(all_losses[-1])\n",
        "epoch_ticks = [1, 20, 40, 60, 80, 100]\n",
        "newpos = [epoch2iter(e) for e in epoch_ticks]\n",
        "ax2 = ax.twiny()\n",
        "ax2.set_xticks(newpos)\n",
        "ax2.set_xticklabels(epoch_ticks)\n",
        "ax2.xaxis.set_ticks_position('bottom')\n",
        "ax2.xaxis.set_label_position('bottom')\n",
        "ax2.spines['bottom'].set_position(('outward', 60))\n",
        "ax2.set_xlabel('Epoch', size=15)\n",
        "ax2.set_xlim(ax.get_xlim())\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
        "\n",
        "# 판별자의 출력\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "d_vals_real = [item[0] for item in itertools.chain(*all_d_vals)]\n",
        "d_vals_fake = [item[1] for item in itertools.chain(*all_d_vals)]\n",
        "plt.plot(d_vals_real, alpha=0.75, label=r'Real: $D(\\mathbf{x})$')\n",
        "plt.plot(d_vals_fake, alpha=0.75, label=r'Fake: $D(G(\\mathbf{z}))$')\n",
        "plt.legend(fontsize=20)\n",
        "ax.set_xlabel('Iteration', size=15)\n",
        "ax.set_ylabel('Discriminator output', size=15)\n",
        "\n",
        "ax2 = ax.twiny()\n",
        "ax2.set_xticks(newpos)\n",
        "ax2.set_xticklabels(epoch_ticks)\n",
        "ax2.xaxis.set_ticks_position('bottom')\n",
        "ax2.xaxis.set_label_position('bottom')\n",
        "ax2.spines['bottom'].set_position(('outward', 60))\n",
        "ax2.set_xlabel('Epoch', size=15)\n",
        "ax2.set_xlim(ax.get_xlim())\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "061eqbzo0R4n"
      },
      "outputs": [],
      "source": [
        "# 생성자의 출력 = 합성된 이미지가 어떻게 변하는지?\n",
        "selected_epochs = [1, 2, 4, 10, 50, 100]\n",
        "fig = plt.figure(figsize=(10, 14))\n",
        "for i, e in enumerate(selected_epochs):\n",
        "    for j in range(5):\n",
        "        ax = fig.add_subplot(6, 5, i*5+j+1)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if j == 0:\n",
        "            ax.text(\n",
        "                -0.06, 0.5, 'Epoch {}'.format(e),\n",
        "                rotation=90, size=18, color='red',\n",
        "                horizontalalignment='right',\n",
        "                verticalalignment='center',\n",
        "                transform=ax.transAxes)\n",
        "            \n",
        "        image= epoch_samples[e-1][j]\n",
        "        ax.imshow(image, cmap='gray_r')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "ex_chapter17_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
